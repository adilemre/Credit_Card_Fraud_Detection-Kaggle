{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import precision_score,recall_score\npd.options.display.float_format = '{:,.2f}'.format\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"User Defined Functions"},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"def calculate_statistics_and_make_a_plot(input_data):\n    '''this function calculates descriptive statistics of a numeric variable and makes a plot'''\n    print(input_data.describe())                                                   #prints descriptive statistiscs\n    draw_graph(input_data)                                                         #draw a graph\n    \n    \ndef draw_graph(dt):\n    ''' this function makes a histogram plot'''\n    row_size=22                                                                    #setting row size\n    col_size=6                                                                     # setting column size\n    fig=plt.figure(figsize=(row_size,col_size))                                    #creating a figure\n    ax1=fig.add_subplot(1,2,1)                                                     #adding a subplot\n    ax1=plt.hist(dt)                                                               #creating a histogram\n    plt.title('{} Distribution'.format(dt.name))                                   #setting a title \n    plt.xlabel(dt.name)                                                            # setting x-axis label\n    plt.ylabel('Count')                                                            # setting y-axis label\n    \ndef calculate_statistics_and_make_a_plot_without_extremes(input_data,lower_threshold=0.01,upper_threshold=0.99):\n    '''this function calculates descriptive statistics of a numeric variable and makes a plot without top and bottom %1 data'''\n    mask=np.logical_and(input_data>input_data.quantile(lower_threshold),           #creating a list for spliting extreme values from the rest\n                        input_data<input_data.quantile(upper_threshold))\n    calculate_statistics_and_make_a_plot(input_data.loc[mask])                     #calculating descriptive statistics and making plot\n            \ndef mark_extreme_values(input_data,col_list,lower_threshold=0.01,upper_threshold=0.99):\n    '''this function marks extreme values.'''\n    for col in col_list:                                                           #creating a for loop runs over the col list\n        new_col_name=col+'_Is_Extreme'                                             #creating new name for new feature\n        input_data[new_col_name]=np.logical_or(input_data[col]<input_data[col].quantile(lower_threshold),  #marking extreme values on a new column\n                                                input_data[col]>input_data[col].quantile(upper_threshold))\n        input_data[new_col_name]=input_data[new_col_name]*1                        #converting boolean values to numeric\n        \ndef replace_extremes(input_data,col_list,lower_threshold=0.01,upper_threshold=0.99):\n    ''' this function replaces extreme values'''\n    for col in col_list:                                                           #creating a for loop runs over the col list\n        new_value=input_data[col].median()                                         #calculating the median value\n        is_extreme=np.logical_or(input_data[col]<input_data[col].quantile(lower_threshold),            #creating a list for spliting extreme values from the rest\n                             input_data[col]>input_data[col].quantile(upper_threshold))\n        input_data[col][is_extreme]=new_value                                      #replacing extreme values with the median value\n    return input_data\n        \ndef remove_extremes(input_data,col_list,lower_threshold=0.01,upper_threshold=0.99):\n    '''this function removes extreme values'''\n    for col in col_list:                                                           #creating a for loop runs over the col list\n        is_extreme=np.logical_or(input_data[col]<input_data[col].quantile(lower_threshold),            #creating a list for spliting extreme values from the rest\n                             input_data[col]>input_data[col].quantile(upper_threshold))\n        input_data.drop(input_data[is_extreme].index, axis=0,inplace=True)         #removing rows with extreme values\n    return input_data\n        \ndef standardize_a_column(data,column):\n    '''this function transforms a feature to a standardized feature'''\n    data[column]=StandardScaler().fit_transform(data[column].values.reshape(-1,1))  #standardizing a column\n    return None\n\ndef split_dataset(input_df,col_name):\n    '''this function splits the dataset as X and Y'''\n    X=input_df.drop(columns=[col_name]).to_numpy()                                  #extracting X\n    y=input_df[col_name].to_numpy()                                                 #extracting X\n    return X,y\n\ndef precision_recall_score(classifier,X,y):\n    '''this function calculates precision and recall score'''\n    from sklearn.metrics import precision_score,recall_score\n    y_pred=classifier.predict(X)                                                    #predict y values\n    return precision_score(y_true=y,y_pred=y_pred),recall_score(y_true=y,y_pred=y_pred)               #calculating precision and recall scores","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"#Reading data\ndataset=pd.read_csv('/kaggle/input/creditcardfraud/creditcard.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1-Explanatory Data Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Time\ncalculate_statistics_and_make_a_plot(dataset['Time'])                                            #calculating descriptive statistics and making graph","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Converting time feature to hours \ndataset['Time-Hour']=[i%24 for i in dataset['Time']/3600]                                        #converting seconds to hours\ndataset['Time-Hour']=dataset['Time-Hour'].astype(int)                                            #converting it to integer\ndataset.drop(columns=['Time'],inplace=True)                                                      #dropping the old time feature. After having time-hour feature, no need to keep this feature any more.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#V1\nprint('Descriptive Statistics\\n')\nprint('Original Data\\n')\ncalculate_statistics_and_make_a_plot(dataset['V1'])\nprint('\\nData Without Extremes\\n')\ncalculate_statistics_and_make_a_plot_without_extremes(dataset['V1'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#V2\nprint('Descriptive Statistics\\n')\nprint('Original Data\\n')\ncalculate_statistics_and_make_a_plot(dataset['V2'])\nprint('\\nData Without Extremes\\n')\ncalculate_statistics_and_make_a_plot_without_extremes(dataset['V2'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#V3\nprint('Descriptive Statistics\\n')\nprint('Original Data\\n')\ncalculate_statistics_and_make_a_plot(dataset['V3'])\nprint('\\nData Without Extremes\\n')\ncalculate_statistics_and_make_a_plot_without_extremes(dataset['V3'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#V4\nprint('Descriptive Statistics\\n')\nprint('Original Data\\n')\ncalculate_statistics_and_make_a_plot(dataset['V4'])\nprint('\\nData Without Extremes\\n')\ncalculate_statistics_and_make_a_plot_without_extremes(dataset['V4'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#V5\nprint('Descriptive Statistics\\n')\nprint('Original Data\\n')\ncalculate_statistics_and_make_a_plot(dataset['V5'])\nprint('\\nData Without Extremes\\n')\ncalculate_statistics_and_make_a_plot_without_extremes(dataset['V5'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#V6\nprint('Descriptive Statistics\\n')\nprint('Original Data\\n')\ncalculate_statistics_and_make_a_plot(dataset['V6'])\nprint('\\nData Without Extremes\\n')\ncalculate_statistics_and_make_a_plot_without_extremes(dataset['V6'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#V7\nprint('Descriptive Statistics\\n')\nprint('Original Data\\n')\ncalculate_statistics_and_make_a_plot(dataset['V7'])\nprint('\\nData Without Extremes\\n')\ncalculate_statistics_and_make_a_plot_without_extremes(dataset['V7'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#V8\nprint('Descriptive Statistics\\n')\nprint('Original Data\\n')\ncalculate_statistics_and_make_a_plot(dataset['V8'])\nprint('\\nData Without Extremes\\n')\ncalculate_statistics_and_make_a_plot_without_extremes(dataset['V8'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#V9\nprint('Descriptive Statistics\\n')\nprint('Original Data\\n')\ncalculate_statistics_and_make_a_plot(dataset['V9'])\nprint('\\nData Without Extremes\\n')\ncalculate_statistics_and_make_a_plot_without_extremes(dataset['V9'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#V10\nprint('Descriptive Statistics\\n')\nprint('Original Data\\n')\ncalculate_statistics_and_make_a_plot(dataset['V10'])\nprint('\\nData Without Extremes\\n')\ncalculate_statistics_and_make_a_plot_without_extremes(dataset['V10'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#V11\nprint('Descriptive Statistics\\n')\nprint('Original Data\\n')\ncalculate_statistics_and_make_a_plot(dataset['V11'])\nprint('\\nData Without Extremes\\n')\ncalculate_statistics_and_make_a_plot_without_extremes(dataset['V11'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#V12\nprint('Descriptive Statistics\\n')\nprint('Original Data\\n')\ncalculate_statistics_and_make_a_plot(dataset['V12'])\nprint('\\nData Without Extremes\\n')\ncalculate_statistics_and_make_a_plot_without_extremes(dataset['V12'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#V13\nprint('Descriptive Statistics\\n')\nprint('Original Data\\n')\ncalculate_statistics_and_make_a_plot(dataset['V13'])\nprint('\\nData Without Extremes\\n')\ncalculate_statistics_and_make_a_plot_without_extremes(dataset['V13'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#V14\nprint('Descriptive Statistics\\n')\nprint('Original Data\\n')\ncalculate_statistics_and_make_a_plot(dataset['V14'])\nprint('\\nData Without Extremes\\n')\ncalculate_statistics_and_make_a_plot_without_extremes(dataset['V14'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#V15\nprint('Descriptive Statistics\\n')\nprint('Original Data\\n')\ncalculate_statistics_and_make_a_plot(dataset['V15'])\nprint('\\nData Without Extremes\\n')\ncalculate_statistics_and_make_a_plot_without_extremes(dataset['V15'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#V16\nprint('Descriptive Statistics\\n')\nprint('Original Data\\n')\ncalculate_statistics_and_make_a_plot(dataset['V16'])\nprint('\\nData Without Extremes\\n')\ncalculate_statistics_and_make_a_plot_without_extremes(dataset['V16'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#V17\nprint('Descriptive Statistics\\n')\nprint('Original Data\\n')\ncalculate_statistics_and_make_a_plot(dataset['V17'])\nprint('\\nData Without Extremes\\n')\ncalculate_statistics_and_make_a_plot_without_extremes(dataset['V18'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#V18\nprint('Descriptive Statistics\\n')\nprint('Original Data\\n')\ncalculate_statistics_and_make_a_plot(dataset['V18'])\nprint('\\nData Without Extremes\\n')\ncalculate_statistics_and_make_a_plot_without_extremes(dataset['V18'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#V19\nprint('Descriptive Statistics\\n')\nprint('Original Data\\n')\ncalculate_statistics_and_make_a_plot(dataset['V19'])\nprint('\\nData Without Extremes\\n')\ncalculate_statistics_and_make_a_plot_without_extremes(dataset['V19'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#V20\nprint('Descriptive Statistics\\n')\nprint('Original Data\\n')\ncalculate_statistics_and_make_a_plot(dataset['V20'])\nprint('\\nData Without Extremes\\n')\ncalculate_statistics_and_make_a_plot_without_extremes(dataset['V20'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#V21\nprint('Descriptive Statistics\\n')\nprint('Original Data\\n')\ncalculate_statistics_and_make_a_plot(dataset['V21'])\nprint('\\nData Without Extremes\\n')\ncalculate_statistics_and_make_a_plot_without_extremes(dataset['V21'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#V22\nprint('Descriptive Statistics\\n')\nprint('Original Data\\n')\ncalculate_statistics_and_make_a_plot(dataset['V22'])\nprint('\\nData Without Extremes\\n')\ncalculate_statistics_and_make_a_plot_without_extremes(dataset['V22'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#V23\nprint('Descriptive Statistics\\n')\nprint('Original Data\\n')\ncalculate_statistics_and_make_a_plot(dataset['V23'])\nprint('\\nData Without Extremes\\n')\ncalculate_statistics_and_make_a_plot_without_extremes(dataset['V23'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#V24\nprint('Descriptive Statistics\\n')\nprint('Original Data\\n')\ncalculate_statistics_and_make_a_plot(dataset['V24'])\nprint('\\nData Without Extremes\\n')\ncalculate_statistics_and_make_a_plot_without_extremes(dataset['V24'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#V25\nprint('Descriptive Statistics\\n')\nprint('Original Data\\n')\ncalculate_statistics_and_make_a_plot(dataset['V25'])\nprint('\\nData Without Extremes\\n')\ncalculate_statistics_and_make_a_plot_without_extremes(dataset['V25'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#V26\nprint('Descriptive Statistics\\n')\nprint('Original Data\\n')\ncalculate_statistics_and_make_a_plot(dataset['V26'])\nprint('\\nData Without Extremes\\n')\ncalculate_statistics_and_make_a_plot_without_extremes(dataset['V26'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#V27\nprint('Descriptive Statistics\\n')\nprint('Original Data\\n')\ncalculate_statistics_and_make_a_plot(dataset['V27'])\nprint('\\nData Without Extremes\\n')\ncalculate_statistics_and_make_a_plot_without_extremes(dataset['V27'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#V28\nprint('Descriptive Statistics\\n')\nprint('Original Data\\n')\ncalculate_statistics_and_make_a_plot(dataset['V28'])\nprint('\\nData Without Extremes\\n')\ncalculate_statistics_and_make_a_plot_without_extremes(dataset['V28'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Amount\nprint('Descriptive Statistics\\n')\nprint('Original Data\\n')\ncalculate_statistics_and_make_a_plot(dataset['Amount'])\nprint('\\nData Without Extremes\\n')\ncalculate_statistics_and_make_a_plot_without_extremes(dataset['Amount'],lower_threshold=0,upper_threshold=0.9)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Target Variable\nprint('Value Counts')\nprint(dataset['Class'].value_counts())\nprint('\\nFraund Ratio:{:.3f}'.format(dataset['Class'].mean()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2- Preparing Data for Predictive Modelling"},{"metadata":{"trusted":true},"cell_type":"code","source":"features_with_extreme_values=['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11','V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21',\n                              'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount']\n#dataset_v1:Amount feature is standardized.\ndataset_v1=dataset.copy()                                                                         # creating a copy of dataset.\nstandardize_a_column(data=dataset_v1,column='Amount')                                             # standardizing \"Amount\" feature.\n\n#dataset_v2\n#Amount feature is standardized+Extreme values are flagged\ndataset_v2=dataset.copy()                                                                         # creating a copy of dataset.\nstandardize_a_column(data=dataset_v2,column='Amount')                                             # standardizing \"Amount\" feature.\nmark_extreme_values(input_data=dataset_v2,                                                        # marking extreme values\n                    col_list=features_with_extreme_values,\n                    lower_threshold=0,upper_threshold=0.99)\n#dataset_v3\n#Amount feature is standardized+Extreme values are removed\ndataset_v3=dataset.copy()                                                                         # creating a copy of dataset.\nstandardize_a_column(data=dataset_v3,column='Amount')                                             # standardizing \"Amount\" feature.\ndataset_v3_refined=remove_extremes(dataset_v3,features_with_extreme_values)                       # removing extreme values\n\n#dataset_v4\n#Amount feature is standardized+Extreme values are replaced with median.\ndataset_v4=dataset_v2.copy()                                                                      # creating a copy of dataset.\nstandardize_a_column(data=dataset_v4,column='Amount')                                             # standardizing \"Amount\" feature.\nreplace_extremes(dataset_v4,features_with_extreme_values)                                         # replacing extreme values with median","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3- Predictive Modelling"},{"metadata":{"trusted":true},"cell_type":"code","source":"#in this section, I work on 4 different datasets and find classifiers to predict fraud cases. \n#As I am doing that, I also compare effect of different extreme value replacement techniques on predictive modelling process.\n#I have run and found parameters of logistic regression and decision tree on my own with grid search cv and use them directly below.\n\nclassifier_dict=pd.DataFrame(columns=['Classifier','Precision_score','Recall_score'])             #creating a dataframe to store models' results\n\n#1st dataset: dataset_v1 --> dataset + Amount feature standardizated and no extreme values modification\nX,y=split_dataset(dataset_v1,'Class')                                                             #splitting the features as dependent(y) and independent variables(X).\nlogistic_regression_1=LogisticRegression(C=0.1,class_weight=None, dual=False,                     #creating a logistic regression classifier\n                                         fit_intercept=True,intercept_scaling=1,l1_ratio=0.1, \n                                         max_iter=10,multi_class='warn', n_jobs=None, \n                                         penalty='elasticnet',random_state=None, solver='saga', \n                                         tol=0.0001,verbose=0,warm_start=False)\nlogistic_regression_1.fit(X,y)                                                                    #fitting the logistic regression classifier\nprecision_score,recall_score=precision_recall_score(logistic_regression_1,X,y)                    #calculating precision (hit ratio) and recall (get ratio) scores\nclassifier_dict.loc['logistic_regression_1']=[logistic_regression_1,precision_score,recall_score] #saving the results in the dataframe\ndecision_tree_1=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=4,          #creating a decision tree classifier\n                                       max_features=0.8, max_leaf_nodes=None,\n                                       min_impurity_decrease=0.0, min_impurity_split=None,\n                                       min_samples_leaf=1, min_samples_split=0.001,\n                                       min_weight_fraction_leaf=0.0, presort=False,\n                                       random_state=123, splitter='best')\ndecision_tree_1.fit(X,y)                                                                          #fitting the decision tree classifier\nprecision_score,recall_score=precision_recall_score(decision_tree_1,X,y)                          #calculating precision (hit ratio) and recall (get ratio) scores\nclassifier_dict.loc['decision_tree_1']=[decision_tree_1,precision_score,recall_score]             #saving results in the dataframe\n\n#2nd dataset: dataset_v2  --> dataset + Amount feature standardized + Extreme values flagged\nX,y=split_dataset(dataset_v2,'Class')                                                             #splitting the features as dependent(y) and independent variables(X).\nlogistic_regression_2=LogisticRegression(C=100, class_weight=None, dual=False,                    #creating a logistic regression classifier \n                                       fit_intercept=True, intercept_scaling=1, l1_ratio=0.7, \n                                       max_iter=10, multi_class='warn', n_jobs=None, \n                                       penalty='elasticnet', random_state=None, solver='saga', \n                                       tol=0.0001, verbose=0, warm_start=False)\nlogistic_regression_2.fit(X,y)                                                                    #fitting the logistic regression classifier\nprecision_score,recall_score=precision_recall_score(logistic_regression_2,X,y)                    #calculating precision (hit ratio) and recall (get ratio) scores\nclassifier_dict.loc['logistic_regression_2']=[logistic_regression_2,precision_score,recall_score] #saving the results in the dataframe\ndecision_tree_2=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=4,          #creating a decision tree classifier\n                                       max_features=0.5, max_leaf_nodes=None,\n                                       min_impurity_decrease=0.0, min_impurity_split=None,\n                                       min_samples_leaf=1, min_samples_split=0.001,\n                                       min_weight_fraction_leaf=0.0, presort=False,\n                                       random_state=123, splitter='best')\ndecision_tree_2.fit(X,y)                                                                          #fitting the decision tree classifier\nprecision_score,recall_score=precision_recall_score(decision_tree_2,X,y)                          #calculating precision (hit ratio) and recall (get ratio) scores\nclassifier_dict.loc['decision_tree_2']=[decision_tree_2,precision_score,recall_score]             #saving results in the dataframe\n\n#3rd dataset: dataset_v3 --> dataset+Amount feature standardized + Extreme values are removed\nX,y=split_dataset(dataset_v3,'Class')                                                             #splitting the features as dependent(y) and independent variables(X).\nlogistic_regression_3=LogisticRegression(C=0.01, class_weight=None, dual=False,                   #creating a logistic regression classifier  \n                                       fit_intercept=True, intercept_scaling=1, l1_ratio=0.1, \n                                       max_iter=10, multi_class='warn', n_jobs=None, \n                                       penalty='elasticnet', random_state=None, solver='saga', \n                                       tol=0.0001, verbose=0, warm_start=False)\n\nlogistic_regression_3.fit(X,y)                                                                    #fitting the logistic regression classifier\nprecision_score,recall_score=precision_recall_score(logistic_regression_3,X,y)                    #calculating precision (hit ratio) and recall (get ratio) scores\nclassifier_dict.loc['logistic_regression_3']=[logistic_regression_3,precision_score,recall_score] #saving the results in the dataframe\ndecision_tree_3=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=2,          #creating a decision tree classifier\n                                       max_features=0.1, max_leaf_nodes=None,\n                                       min_impurity_decrease=0.0, min_impurity_split=None,\n                                       min_samples_leaf=1, min_samples_split=0.1,\n                                       min_weight_fraction_leaf=0.0, presort=False,\n                                       random_state=123, splitter='best')\ndecision_tree_3.fit(X,y)                                                                          #fitting the decision tree classifier\nprecision_score,recall_score=precision_recall_score(decision_tree_3,X,y)                          #calculating precision (hit ratio) and recall (get ratio) scores\nclassifier_dict.loc['decision_tree_3']=[decision_tree_3,precision_score,recall_score]             #saving results in the dataframe\n\n\n\n#4rd dataset: dataset_v4--> dataset+Amount feature standardized + Extreme values are replaced with median.\nX,y=split_dataset(dataset_v4,'Class')                                                             #splitting the features as dependent(y) and independent variables(X).\nlogistic_regression=LogisticRegression(C=10, class_weight=None, dual=False,                       #creating a logistic regression classifier  \n                                       fit_intercept=True, intercept_scaling=1, l1_ratio=0.7, \n                                       max_iter=10, multi_class='warn', n_jobs=None, \n                                       penalty='elasticnet', random_state=None, solver='saga', \n                                       tol=0.0001, verbose=0, warm_start=False)\nlogistic_regression.fit(X,y)                                                                      #fitting the logistic regression classifier\nprecision_score,recall_score=precision_recall_score(logistic_regression,X,y)                      #calculating precision (hit ratio) and recall (get ratio) scores\nclassifier_dict.loc['logistic_regression_4']=[logistic_regression,precision_score,recall_score]   #saving the results in the dataframe\ndecision_tree_4=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=6,          #creating a decision tree classifier\n                                       max_features=0.8, max_leaf_nodes=None,\n                                       min_impurity_decrease=0.0, min_impurity_split=None,\n                                       min_samples_leaf=1, min_samples_split=0.001,\n                                       min_weight_fraction_leaf=0.0, presort=False,\n                                       random_state=123, splitter='best')\ndecision_tree_4.fit(X,y)                                                                          #fitting the decision tree classifier\nprecision_score,recall_score=precision_recall_score(decision_tree_4,X,y)                          #calculating precision (hit ratio) and recall (get ratio) scores\nclassifier_dict.loc['decision_tree_4']=[decision_tree_4,precision_score,recall_score]             #saving results in the dataframe","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#creating a plot to compare results easier\n#creating a new feature for dataset information.\nclassifier_dict.loc['logistic_regression_1','Data_Process_Summary']='Amount feature is standardized'\nclassifier_dict.loc['decision_tree_1','Data_Process_Summary']='Amount feature is standardized'\nclassifier_dict.loc['logistic_regression_2','Data_Process_Summary']='Amount feature is standardized+Extreme values are flagged'\nclassifier_dict.loc['decision_tree_2','Data_Process_Summary']='Amount feature is standardized+Extreme values are flagged'\nclassifier_dict.loc['logistic_regression_3','Data_Process_Summary']='Amount feature is standardized+Extreme values are removed'\nclassifier_dict.loc['decision_tree_3','Data_Process_Summary']='Amount feature is standardized are removed'\nclassifier_dict.loc['logistic_regression_4','Data_Process_Summary']='Amount feature is standardized+Extreme values are flagged and replaced'\nclassifier_dict.loc['decision_tree_4','Data_Process_Summary']='Amount feature is standardized+Extreme values are flagged and replaced'\n#make a plot\nplt.figure(figsize=(12,8))\nsns.scatterplot(x='Precision_score',y='Recall_score',data=classifier_dict,style='Data_Process_Summary',s=100)\nplt.title('Classifiers Precision-Recall Plot')\nplt.xlabel('Precision')\nplt.ylabel('Recall');","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}
